{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f93921-3263-4f59-82a0-9a3c0c2de2d3",
   "metadata": {},
   "source": [
    "## Creating Index with PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5840ab61-aeae-4312-b77b-ddf4e9435595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All necessary imports\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1\n",
    "from google.protobuf import struct_pb2\n",
    "import grpc\n",
    "from google.cloud.aiplatform.matching_engine._protos import match_service_pb2\n",
    "from google.cloud.aiplatform.matching_engine._protos import (\n",
    "    match_service_pb2_grpc,)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464130c1-14ef-43d4-b547-d6f5889f8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring Variables\n",
    "project_id = \"<enter project id>\"\n",
    "dataset_id = \"<enter the dataset id of BQ>\n",
    "table_id = \"<enter the table id of BQ>\n",
    "location = \"<enter location of project>\"\n",
    "processor_id = \"<enter OCR processor id>\"\n",
    "processor_version = \"<enter OCR processor version>\"\n",
    "mime_type = \"<enter mime type of your file format, for pdf- application/pdf>\"\n",
    "full_table_id = f\"{project_id}.{dataset_id}.{table_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65738b7-3c93-4b25-8afb-24ff38aa9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all pdf files\n",
    "import glob\n",
    "import json\n",
    "files = glob.glob(\"<enter location of pdf files available>\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389d064-e0a4-4b94-bfcf-b95b014c22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate embeddings for the text using gecko model\n",
    "final_embedding = []\n",
    "def text_embedding(vocab_list):\n",
    "    \"\"\"\n",
    "    Text embedding with a Large Language Model.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    vocab_list : List\n",
    "                 Contains list of strings\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    final_embedding : List\n",
    "                      List of embedding vectors\n",
    "    \"\"\"\n",
    "    start,end = 0,5\n",
    "    while start<end:\n",
    "        model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "        embeddings = model.get_embeddings(vocab_list[start:end])\n",
    "        final_embedding.extend(embeddings)\n",
    "        start = end\n",
    "        end = end+5\n",
    "        if end<=len(vocab_list):\n",
    "            end=end\n",
    "        else:\n",
    "            end=len(vocab_list)\n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd286c95-4bfd-417f-9699-7dd47cea72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sent(doc_text):\n",
    "    \"\"\"\n",
    "    This function is used to create chunks of\n",
    "    2000 tokens out of an entire document\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    doc_text : String\n",
    "               Contains entire document text\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    chunked_sent : List\n",
    "                   List of strings containing chunked\n",
    "                   sentences\n",
    "    \"\"\"\n",
    "    chunked_sent = []\n",
    "    start, end = 0, 2000\n",
    "    word_list = doc_text.split()\n",
    "    for i in range(len(word_list)):\n",
    "        temp_sent = \" \".join(word_list[start:end])\n",
    "        chunked_sent.append(temp_sent)\n",
    "        if end>=len(word_list):\n",
    "            return chunked_sent\n",
    "        else:\n",
    "            start = end\n",
    "            end = end+2000\n",
    "    return chunked_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9020e-f1be-4cc3-b645-7321ca0f6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for file_path in files:\n",
    "    file_path = file_path\n",
    "    doc_text = process_document_ocr_sample(project_id, location, processor_id, processor_version, file_path, mime_type)\n",
    "    \n",
    "    #Chunking to 2000 tokens for each doc\n",
    "    chunked_sent = chunk_sent(doc_text)\n",
    "    vocab_list.extend(chunked_sent)\n",
    "    \n",
    "#Send the complete chunked sentences list to Palm API to get the vectors\n",
    "embedding_list = text_embedding(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa01a0-38e3-4008-808c-79ce379f5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to store the index and its corresponding value in BigQuery\n",
    "bq_client = bigquery.Client()\n",
    "destination = bigquery.table.Table.from_string(full_table_id)\n",
    "\n",
    "for index,value in enumerate(vocab_list):\n",
    "    bq_row = [{\n",
    "                \"Index\": str(index),\n",
    "                \"Value\": str(value),\n",
    "            }]\n",
    "    bq_client.insert_rows_json(destination, bq_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0febe213-f412-45b0-bc93-46ec0e2f24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the JSONL file to create Index\n",
    "final_list = []\n",
    "with open(\"index_file.json\", \"w\") as f:\n",
    "    for i in range(len(embedding_list)):\n",
    "        val_dict = {}\n",
    "        val_dict[\"id\"] = str(i)\n",
    "        val_dict[\"embedding\"] = embedding_list[i].values\n",
    "        f.writelines(json.dumps(val_dict)+ \"\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe35b93-e1c9-496c-8280-1cdd92243630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Index in Vertex Matching Engine\n",
    "aiplatform.init(project=project_id, location=location)\n",
    "index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=\"<enter a display name for the index>\",\n",
    "    contents_delta_uri=\"<gs path of jsonl file created above>\",\n",
    "    dimensions=768,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"COSINE_DISTANCE\",\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=7,\n",
    "    description=\"<description of your index created>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f9a60-2a85-410c-9e69-5feda05fb59d",
   "metadata": {},
   "source": [
    "## Creating an Endpoint and deploying the index on the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d0886-edac-4012-a465-48b7b8c40266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deploying to an endpoint\n",
    "#Create Endpoint\n",
    "REGION = \"<enter region where you want to create endpoint>\"\n",
    "ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "PROJECT_ID = project_id\n",
    "PARENT = \"projects/{}/locations/{}\".format(PROJECT_ID, REGION)\n",
    "\n",
    "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
    "\n",
    "NETWORK_NAME = \"<VPC network name>\"\n",
    "\n",
    "VPC_NETWORK_NAME = \"projects/{}/global/networks/{}\".format(PROJECT_NUMBER, NETWORK_NAME)\n",
    "VPC_NETWORK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a192e-b4f8-41b1-a9e2-7add1f5b4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an endpoint\n",
    "DISPLAY_NAME=\"<Endpoint display Name>\"\n",
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    description=DISPLAY_NAME,\n",
    "    project = project_id,\n",
    "    network=VPC_NETWORK_NAME,\n",
    "    location = REGION\n",
    "    #IMPORTANT if you want to use a public endpoint you need to use aiplatform_v1beta1 when query or inserting vectors\n",
    "    # https://cloud.google.com/vertex-ai/docs/matching-engine/deploy-index-public\n",
    "    # public_endpoint_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a398ffb-fc51-428a-bf65-890bf337a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_RESOURCE_NAME = \"projects/<project number>/locations/<region>/indexes/<index id>\"\n",
    "index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)\n",
    "\n",
    "ENDPOINT_RESOURCE_NAME = \"projects/<project number>/locations/<region>/indexEndpoints/<endpoint id>\"\n",
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_name=ENDPOINT_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95ed68-fb39-41e0-bc5c-a9dc252e2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deploy index to the created endpoint\n",
    "deployed_index = index_endpoint.deploy_index(\n",
    "    index=index, deployed_index_id=DISPLAY_NAME.replace('-','_')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71ba1e-b680-42d9-819b-6d77705109f1",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cbe01-b5a1-4064-ab6b-cf4ef4353185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference embedding function\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "def text_embedding(vocab_list):\n",
    "    \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    embeddings = model.get_embeddings(vocab_list)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735b606-4dff-43dd-886d-7bc857fb5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Write a Query\"]\n",
    "query_embeddings = text_embedding(query)\n",
    "response = index_endpoint.match(\n",
    "    deployed_index_id=\"<deployed_index_id_name>\",\n",
    "    queries=[query_embeddings[0].values],\n",
    "    num_neighbors=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e21e9b-f0f5-4fbf-92a7-1f945d5bd28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a query to fetch value from BQ against the index\n",
    "client = bigquery.Client()\n",
    "index = int(response[0][0].id)\n",
    "QUERY = (\n",
    "    f'SELECT name FROM {full_table_id}'\n",
    "    'WHERE Index = index')\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "for row in rows:\n",
    "    context = row.Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507716a9-55f7-40cb-ba98-bf4fde2dbaa9",
   "metadata": {},
   "source": [
    "## Generating Text based on the context of semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc6203-16a3-4c08-baee-ea2ded59e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Text\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "prompt=f\"\"\"\n",
    "Follow exactly those 3 steps:\n",
    "1. Read the context below and aggregrate this data\n",
    "Context : {context}\n",
    "2. Answer the question using only this context\n",
    "User query: {query[0]}\n",
    "\n",
    "If you don't have any context and are unsure of the answer, reply that you don't know about this topic.\n",
    "\"\"\"\n",
    "\n",
    "model = TextGenerationModel.from_pretrained('text-bison@001')\n",
    "response = model.predict(\n",
    "        prompt,\n",
    "        temperature=0.2,\n",
    "        top_k=40,\n",
    "        top_p=.8,\n",
    "        max_output_tokens=1024,\n",
    ")\n",
    "print(f\"Response: \\n{response.text}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
